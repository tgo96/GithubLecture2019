{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnm0i7J9QjJ4In/LHq5MuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgo96/GithubLecture2019/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fOuTJNJ7X17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_x6vE50N8dJ"
      },
      "source": [
        "# Deep Q-Network(DQN)\n",
        "Deep Q-Network(DQN)は，GoogleのDeep mindチームが2016年に発表した手法で，Q学習におけるQテーブルを用いた行動価値の導出をDCNNを用いた近似関数で代用するのが主な手法の内容である．それまでのQ学習ではすべての状態と行動に組み合わせについての行動価値をQテーブルに記録していたために状態数と行動数の組み合わせが膨大な環境に対して膨大なメモリが必要となる問題を抱えていました．これに対してDQNはQテーブルそのものをDCNNで代用することにより膨大な行動数と状態数の組み合わせが存在する環境に対しても学習を行うことが可能となり，atari2600のゲーム環境において人間を超えるスコアを出すことに成功している．また，DQNにはその他にも強化学習における学習の安定性獲得のためにExperience replay，Target Q-Network，reward_clippingなどの工夫がなされている．行動価値関数(Q値)を用いて行動を決定し，最適行動価値関数になるよう更新を行っていく手法は，価値ベースの手法と呼ばれている．派生手法には，DDQN，Ape-X，R2D2などの手法がある．\n",
        "\\\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1J7YPwb1iMd4eKo3YKLc7Rk4XG-vT7hoz\" width = 70%>\n",
        "\\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQGfxWYK_4O"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "\n",
        "### モジュールの追加インストール\n",
        "下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0AFod2df5j5"
      },
      "source": [
        "!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKpJ1mtDFoDo"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるopenAI Gym（gym）をインポートする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0oGGaN1FoH2"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 使用するデバイス（GPU or CPU）の決定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Use device:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo5u_p4SFoMa"
      },
      "source": [
        "## OpenAI GymによるPongの環境の定義\n",
        " [OpenAI Gym](https://github.com/openai/gym) は，様々な種類の環境を提供しているモジュールです．\n",
        " \n",
        " 今回はgymで利用できるatariゲームであるPongを実行します．\n",
        " まず，gym.make関数で実行したい環境を指定します．\n",
        " その後，reset関数を実行することで，環境を初期化します．\n",
        " \n",
        "Pong環境は，パドルを操作してボールが自分の陣地に入らないように打ち返すゲームです．Pong環境において現在の状態を確認するためにゲームの画面情報が与えられており，`observation_space`という変数で確認することができます．\n",
        "また，`action_space`という変数で，エージェントが取ることのできる行動の数を確認することができます．\n",
        "Pongの場合は，パドルを上下どちらかに移動させるという行動を取るため，行動の数は2となっています．\n",
        "Pongのゲーム概要は，相手の陣地にボールを入れることで得点を獲得し，ボールを自分の陣地に入れられることで得点を取られます．どちらかのプレイヤーが21点取った時点で終了となります．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsZ__suDFoQd"
      },
      "source": [
        "# 環境の指定\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 環境の初期化\n",
        "obs = env.reset()\n",
        "#env.render()\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "print('initial observation:', obs)\n",
        "\n",
        "# 行動の決定と決定した行動の入力\n",
        "action = env.action_space.sample()\n",
        "obs, r, done, info = env.step(action)\n",
        "print('next observation:', obs)\n",
        "print('reward:', r)\n",
        "print('done:', done)\n",
        "print('info:', info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOgbuO2I1bfq"
      },
      "source": [
        "## ネットワーク構造\n",
        "ネットワークモデルを定義します． ここでは，環境からのゲーム画面情報を入力し，行動に対するQ値を出力するようなネットワークを定義するために，畳み込み層3層と全結合層2層から構成されるネットワークとします．\n",
        "\n",
        "入力データのサイズをinput_shape，出力する行動の数をn_actionsとし，ネットワークの作成時に変更できるようにしておきます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh12648qOHXC"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsX8hfGurmBt"
      },
      "source": [
        "## Deep Q-Networkにおける学習工夫の定義\n",
        "\n",
        "Deep Q-Networkでは学習の促進と安定化の為に，いくつ工夫を施して学習を行っています．代表的な工夫としてExperience Replay, Target Q-Network, Reward Clippingと呼ばれる3つの工夫があります．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXY4bzSf3fRA"
      },
      "source": [
        "### Experience Replay\n",
        "DQNは環境の状態を直接観測するのではなく一度Replay Bufferと呼ばれるBufferに経験を格納しておき，学習する際に格納したランダムな経験を取ってくることによって学習を行っています．これをExperience Replayと呼びデータの再利用をおこなうことで，データ効率を高め効率的な学習をおこないます．ここでは，replay memoryの定義を行います．\n",
        "\n",
        "memoryへは，現在の状態，その時に選択された行動，行動によって遷移した状態（次状態），その時の報酬の4種類の情報を1つの経験として蓄積します． まず，Experienceという変数を定義します． ここでは，state, action, reward, done, next_stateが1セットとなるようなデータ構造（辞書オブジェクト）を定義します．\n",
        "\n",
        "その後Experience Bufferクラスを定義します． Experience Bufferクラスでは，memoryへ格納する経験の数（capacity），経験を蓄積するbuffer（buffer）を定義します． append関数では，メモリへ経験を格納します． また，sample関数では，指定したバッチサイズ (batch_size) 分の経験をランダムに選択し，返す関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL0Oolhp47OJ"
      },
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYQi9_LQ5UiF"
      },
      "source": [
        "### Target Q-Network\n",
        "強化学習で誤差を計算する際，目標値として使用される行動価値関数と現在の行動価値関数をそれぞれ別のネットワークの出力を用いて誤差を計算します．目標値の行動価値関数を出力するネットワークは一定周期経過するまで固定したネットワークとし，一定周期で現在のネットワークと同期しながら学習を行う工夫がTarget Q-Networkです．この工夫により，学習の安定化を図ります．\n",
        "\n",
        "target_netを現在のnetと同期するsync_networkを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iod_iK4Yt7tB"
      },
      "source": [
        "def sync_network():\n",
        "    tgt_net.load_state_dict(net.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUL1qa38uol6"
      },
      "source": [
        "### reward clipping\n",
        "学習における報酬値を −1 から 1 の範囲に指定します．これにより,学習における外れ値に対する過剰反応を防ぐことができます.\n",
        "\n",
        "reward clippingは環境に直接ラップするためwrapperクラスを定義します．reward関数では，環境から受け取った報酬を-1から1の範囲にクリップします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL7Kkp-svXyu"
      },
      "source": [
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk9EDXRAt-1I"
      },
      "source": [
        "## その他の学習に必要な処理\n",
        "gymのatari環境で学習の安定化と効率的な学習を行うための処理をいくつか行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53nSLW5xwH70"
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7C3y7rvo9HP"
      },
      "source": [
        "### 学習に必要な処理の適用\n",
        "環境に対して必要となるそれぞれの処理を作成した環境に対して適用します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1G4H7WxpSKA"
      },
      "source": [
        "env = gym.make(\"PongNoFrameskip-v4\")\n",
        "env = ClipRewardEnv(env)\n",
        "env = MaxAndSkipEnv(env)\n",
        "env = FireResetEnv(env)\n",
        "env = ProcessFrame84(env)\n",
        "env = ImageToPyTorch(env)\n",
        "env = BufferWrapper(env, 4)\n",
        "env = ScaledFloatFrame(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23ixNGvKdAB4"
      },
      "source": [
        "## エージェントの定義\n",
        "エージェントが環境に対して行動価値に沿った行動を行い，環境から経験を取得し，Experience Buffer記録にするようにします．\n",
        "\n",
        "エージェントの環境に対する動きのクラスを定義します．play_step関数は，環境にたいして行動を決定する関数です．epsilon-greedy法を用いて一定の割合でランダムに行動選択を行います．それ以外の場合は，使用しているネットワークへ環境情報を入力し，行動を決定します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaGTP_HhdB6m"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net=None, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrH9M2xiPey"
      },
      "source": [
        "## TD誤差の計算\n",
        "強化学習は，TD誤差と呼ばれる次状態の推定の価値と実際に起こした行動から得られる価値の差を用いて学習を行います．次状態の推定の価値を教師あり学習の教師と同じ役割として計算します．DQNはQ学習をもとにしているため，現在の行動価値関数を最適行動価値関数になるように更新を行っていきます．\n",
        "\n",
        "TD誤差の計算を行う関数を定義します．calc_loss関数では，replay_bufferからランダムに取得したbatch分の経験をもとに下記のLoss計算を行います．\n",
        "\n",
        "\\\n",
        "$$\n",
        "L_{\\theta}=\\frac{1}{2}(r+\\gamma \\max_{a'}Q_{\\theta_{i}}(s',a')-Q_{\\theta_{i}}(s,a))^{2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjm67ns-iUYR"
      },
      "source": [
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done_mask] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMkQ1vGVxTUQ"
      },
      "source": [
        "## 学習\n",
        "DQNを用いて学習を行います．学習環境はatari環境のPongゲーム環境を用います．各パラメータを定義します．Experience Replayで利用するReplay Bufferのサイズは50万とし，Replay Bufferが埋まるまでエージェントがランダムで動き，経験を貯めてから学習を行います．\n",
        "\n",
        "学習回数を100万frame(num_frame)分とし，環境の終了条件はどちらかが21点とったら終了としています．また，最適化手法にはRMSprop利用します．\n",
        "\n",
        "学習を開始します． まず，環境を初期化し，経験をReplayBufferへ蓄積します． 十分に蓄積された後，パラメータの更新を行います． また，SNC_TARGET_FRAMESで指定した回数ごとに，target_netのパラメータをnetのパラメータと同じになるようにコピーを行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4vsgARct76r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a06080f-2908-4a7f-d2ce-90478cf0b690"
      },
      "source": [
        "MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 500000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "num_frame = 1000000\n",
        "\n",
        "device = 'cuda:0'\n",
        "\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts = time.time()\n",
        "ts_frame = 0\n",
        "best_mean_reward = None\n",
        "\n",
        "for i in REPLAY_SIZE:\n",
        "    agent.play_step()\n",
        "\n",
        "while frame_idx < num_frame:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, epsilon, device=device)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        mean_reward = np.mean(total_rewards[-100:])\n",
        "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "            frame_idx, len(total_rewards), mean_reward, epsilon,\n",
        "            speed\n",
        "        ))\n",
        "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "            torch.save(net.state_dict(), \"Pong\" + \"-best.dat\")\n",
        "            if best_mean_reward is not None:\n",
        "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
        "            best_mean_reward = mean_reward\n",
        "        if len(total_rewards) == 1000000:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        sync_network()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "790: done 1 games, mean reward -21.000, eps 0.99, speed 516.14 f/s\n",
            "1582: done 2 games, mean reward -21.000, eps 0.98, speed 518.40 f/s\n",
            "2718: done 3 games, mean reward -20.000, eps 0.97, speed 520.04 f/s\n",
            "Best mean reward updated -21.000 -> -20.000, model saved\n",
            "3713: done 4 games, mean reward -20.000, eps 0.96, speed 520.40 f/s\n",
            "4643: done 5 games, mean reward -20.200, eps 0.95, speed 522.99 f/s\n",
            "5423: done 6 games, mean reward -20.333, eps 0.95, speed 523.42 f/s\n",
            "6428: done 7 games, mean reward -20.286, eps 0.94, speed 523.26 f/s\n",
            "7275: done 8 games, mean reward -20.375, eps 0.93, speed 518.14 f/s\n",
            "8246: done 9 games, mean reward -20.444, eps 0.92, speed 506.98 f/s\n",
            "9068: done 10 games, mean reward -20.500, eps 0.91, speed 513.05 f/s\n",
            "10158: done 11 games, mean reward -20.455, eps 0.90, speed 514.66 f/s\n",
            "11078: done 12 games, mean reward -20.417, eps 0.89, speed 513.83 f/s\n",
            "11988: done 13 games, mean reward -20.462, eps 0.88, speed 507.92 f/s\n",
            "12974: done 14 games, mean reward -20.429, eps 0.87, speed 505.81 f/s\n",
            "13755: done 15 games, mean reward -20.467, eps 0.86, speed 500.49 f/s\n",
            "14545: done 16 games, mean reward -20.500, eps 0.85, speed 499.39 f/s\n",
            "15335: done 17 games, mean reward -20.529, eps 0.85, speed 498.67 f/s\n",
            "16125: done 18 games, mean reward -20.556, eps 0.84, speed 486.33 f/s\n",
            "17100: done 19 games, mean reward -20.474, eps 0.83, speed 495.07 f/s\n",
            "17924: done 20 games, mean reward -20.500, eps 0.82, speed 494.02 f/s\n",
            "18937: done 21 games, mean reward -20.429, eps 0.81, speed 485.31 f/s\n",
            "19718: done 22 games, mean reward -20.455, eps 0.80, speed 493.39 f/s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20586: done 23 games, mean reward -20.435, eps 0.79, speed 94.43 f/s\n",
            "21348: done 24 games, mean reward -20.458, eps 0.79, speed 68.69 f/s\n",
            "22406: done 25 games, mean reward -20.440, eps 0.78, speed 68.29 f/s\n",
            "23168: done 26 games, mean reward -20.462, eps 0.77, speed 68.50 f/s\n",
            "23949: done 27 games, mean reward -20.481, eps 0.76, speed 68.79 f/s\n",
            "24859: done 28 games, mean reward -20.500, eps 0.75, speed 68.52 f/s\n",
            "26094: done 29 games, mean reward -20.448, eps 0.74, speed 68.38 f/s\n",
            "26913: done 30 games, mean reward -20.467, eps 0.73, speed 68.24 f/s\n",
            "27971: done 31 games, mean reward -20.419, eps 0.72, speed 67.06 f/s\n",
            "29104: done 32 games, mean reward -20.344, eps 0.71, speed 67.45 f/s\n",
            "29922: done 33 games, mean reward -20.364, eps 0.70, speed 67.45 f/s\n",
            "30703: done 34 games, mean reward -20.382, eps 0.69, speed 67.08 f/s\n",
            "31648: done 35 games, mean reward -20.400, eps 0.68, speed 66.90 f/s\n",
            "32599: done 36 games, mean reward -20.417, eps 0.67, speed 66.88 f/s\n",
            "33507: done 37 games, mean reward -20.432, eps 0.66, speed 66.58 f/s\n",
            "34454: done 38 games, mean reward -20.395, eps 0.66, speed 65.41 f/s\n",
            "35557: done 39 games, mean reward -20.333, eps 0.64, speed 65.64 f/s\n",
            "36650: done 40 games, mean reward -20.275, eps 0.63, speed 67.00 f/s\n",
            "37579: done 41 games, mean reward -20.268, eps 0.62, speed 64.41 f/s\n",
            "38341: done 42 games, mean reward -20.286, eps 0.62, speed 65.42 f/s\n",
            "39311: done 43 games, mean reward -20.302, eps 0.61, speed 64.33 f/s\n",
            "40365: done 44 games, mean reward -20.250, eps 0.60, speed 63.86 f/s\n",
            "41265: done 45 games, mean reward -20.267, eps 0.59, speed 64.33 f/s\n",
            "42310: done 46 games, mean reward -20.217, eps 0.58, speed 63.94 f/s\n",
            "43442: done 47 games, mean reward -20.213, eps 0.57, speed 63.91 f/s\n",
            "44606: done 48 games, mean reward -20.208, eps 0.55, speed 63.46 f/s\n",
            "45965: done 49 games, mean reward -20.143, eps 0.54, speed 63.48 f/s\n",
            "47440: done 50 games, mean reward -20.060, eps 0.53, speed 63.31 f/s\n",
            "48622: done 51 games, mean reward -20.059, eps 0.51, speed 63.14 f/s\n",
            "49895: done 52 games, mean reward -20.019, eps 0.50, speed 62.91 f/s\n",
            "50886: done 53 games, mean reward -20.038, eps 0.49, speed 63.30 f/s\n",
            "52182: done 54 games, mean reward -20.037, eps 0.48, speed 62.75 f/s\n",
            "53744: done 55 games, mean reward -20.000, eps 0.46, speed 63.05 f/s\n",
            "55207: done 56 games, mean reward -19.946, eps 0.45, speed 63.13 f/s\n",
            "Best mean reward updated -20.000 -> -19.946, model saved\n",
            "56609: done 57 games, mean reward -19.912, eps 0.43, speed 62.99 f/s\n",
            "Best mean reward updated -19.946 -> -19.912, model saved\n",
            "57986: done 58 games, mean reward -19.879, eps 0.42, speed 62.22 f/s\n",
            "Best mean reward updated -19.912 -> -19.879, model saved\n",
            "59545: done 59 games, mean reward -19.881, eps 0.40, speed 61.75 f/s\n",
            "60924: done 60 games, mean reward -19.833, eps 0.39, speed 62.13 f/s\n",
            "Best mean reward updated -19.879 -> -19.833, model saved\n",
            "62574: done 61 games, mean reward -19.754, eps 0.37, speed 62.18 f/s\n",
            "Best mean reward updated -19.833 -> -19.754, model saved\n",
            "64504: done 62 games, mean reward -19.710, eps 0.35, speed 61.23 f/s\n",
            "Best mean reward updated -19.754 -> -19.710, model saved\n",
            "66353: done 63 games, mean reward -19.651, eps 0.34, speed 61.07 f/s\n",
            "Best mean reward updated -19.710 -> -19.651, model saved\n",
            "68109: done 64 games, mean reward -19.625, eps 0.32, speed 60.93 f/s\n",
            "Best mean reward updated -19.651 -> -19.625, model saved\n",
            "69622: done 65 games, mean reward -19.600, eps 0.30, speed 60.49 f/s\n",
            "Best mean reward updated -19.625 -> -19.600, model saved\n",
            "71402: done 66 games, mean reward -19.561, eps 0.29, speed 60.42 f/s\n",
            "Best mean reward updated -19.600 -> -19.561, model saved\n",
            "73414: done 67 games, mean reward -19.537, eps 0.27, speed 59.96 f/s\n",
            "Best mean reward updated -19.561 -> -19.537, model saved\n",
            "75404: done 68 games, mean reward -19.544, eps 0.25, speed 60.12 f/s\n",
            "77665: done 69 games, mean reward -19.493, eps 0.22, speed 59.82 f/s\n",
            "Best mean reward updated -19.537 -> -19.493, model saved\n",
            "79784: done 70 games, mean reward -19.457, eps 0.20, speed 59.95 f/s\n",
            "Best mean reward updated -19.493 -> -19.457, model saved\n",
            "81921: done 71 games, mean reward -19.408, eps 0.18, speed 59.62 f/s\n",
            "Best mean reward updated -19.457 -> -19.408, model saved\n",
            "84411: done 72 games, mean reward -19.278, eps 0.16, speed 59.20 f/s\n",
            "Best mean reward updated -19.408 -> -19.278, model saved\n",
            "86696: done 73 games, mean reward -19.192, eps 0.13, speed 58.74 f/s\n",
            "Best mean reward updated -19.278 -> -19.192, model saved\n",
            "89316: done 74 games, mean reward -19.095, eps 0.11, speed 58.37 f/s\n",
            "Best mean reward updated -19.192 -> -19.095, model saved\n",
            "92323: done 75 games, mean reward -18.947, eps 0.08, speed 58.16 f/s\n",
            "Best mean reward updated -19.095 -> -18.947, model saved\n",
            "94974: done 76 games, mean reward -18.855, eps 0.05, speed 57.64 f/s\n",
            "Best mean reward updated -18.947 -> -18.855, model saved\n",
            "96956: done 77 games, mean reward -18.805, eps 0.03, speed 57.34 f/s\n",
            "Best mean reward updated -18.855 -> -18.805, model saved\n",
            "99492: done 78 games, mean reward -18.731, eps 0.02, speed 57.77 f/s\n",
            "Best mean reward updated -18.805 -> -18.731, model saved\n",
            "102250: done 79 games, mean reward -18.658, eps 0.02, speed 57.93 f/s\n",
            "Best mean reward updated -18.731 -> -18.658, model saved\n",
            "104956: done 80 games, mean reward -18.587, eps 0.02, speed 57.63 f/s\n",
            "Best mean reward updated -18.658 -> -18.587, model saved\n",
            "108810: done 81 games, mean reward -18.383, eps 0.02, speed 57.55 f/s\n",
            "Best mean reward updated -18.587 -> -18.383, model saved\n",
            "112547: done 82 games, mean reward -18.171, eps 0.02, speed 57.25 f/s\n",
            "Best mean reward updated -18.383 -> -18.171, model saved\n",
            "115694: done 83 games, mean reward -17.880, eps 0.02, speed 57.27 f/s\n",
            "Best mean reward updated -18.171 -> -17.880, model saved\n",
            "118498: done 84 games, mean reward -17.762, eps 0.02, speed 57.38 f/s\n",
            "Best mean reward updated -17.880 -> -17.762, model saved\n",
            "121317: done 85 games, mean reward -17.682, eps 0.02, speed 57.60 f/s\n",
            "Best mean reward updated -17.762 -> -17.682, model saved\n",
            "124167: done 86 games, mean reward -17.640, eps 0.02, speed 57.83 f/s\n",
            "Best mean reward updated -17.682 -> -17.640, model saved\n",
            "127386: done 87 games, mean reward -17.506, eps 0.02, speed 57.80 f/s\n",
            "Best mean reward updated -17.640 -> -17.506, model saved\n",
            "130058: done 88 games, mean reward -17.193, eps 0.02, speed 57.74 f/s\n",
            "Best mean reward updated -17.506 -> -17.193, model saved\n",
            "132303: done 89 games, mean reward -16.854, eps 0.02, speed 57.44 f/s\n",
            "Best mean reward updated -17.193 -> -16.854, model saved\n",
            "134201: done 90 games, mean reward -16.478, eps 0.02, speed 57.65 f/s\n",
            "Best mean reward updated -16.854 -> -16.478, model saved\n",
            "136124: done 91 games, mean reward -16.121, eps 0.02, speed 57.53 f/s\n",
            "Best mean reward updated -16.478 -> -16.121, model saved\n",
            "138036: done 92 games, mean reward -15.761, eps 0.02, speed 57.42 f/s\n",
            "Best mean reward updated -16.121 -> -15.761, model saved\n",
            "139937: done 93 games, mean reward -15.409, eps 0.02, speed 57.49 f/s\n",
            "Best mean reward updated -15.761 -> -15.409, model saved\n",
            "142335: done 94 games, mean reward -15.138, eps 0.02, speed 57.57 f/s\n",
            "Best mean reward updated -15.409 -> -15.138, model saved\n",
            "144459: done 95 games, mean reward -14.811, eps 0.02, speed 57.91 f/s\n",
            "Best mean reward updated -15.138 -> -14.811, model saved\n",
            "146973: done 96 games, mean reward -14.573, eps 0.02, speed 58.08 f/s\n",
            "Best mean reward updated -14.811 -> -14.573, model saved\n",
            "148993: done 97 games, mean reward -14.258, eps 0.02, speed 57.84 f/s\n",
            "Best mean reward updated -14.573 -> -14.258, model saved\n",
            "151123: done 98 games, mean reward -13.939, eps 0.02, speed 57.90 f/s\n",
            "Best mean reward updated -14.258 -> -13.939, model saved\n",
            "153472: done 99 games, mean reward -13.687, eps 0.02, speed 57.43 f/s\n",
            "Best mean reward updated -13.939 -> -13.687, model saved\n",
            "155348: done 100 games, mean reward -13.360, eps 0.02, speed 57.56 f/s\n",
            "Best mean reward updated -13.687 -> -13.360, model saved\n",
            "157529: done 101 games, mean reward -12.990, eps 0.02, speed 57.58 f/s\n",
            "Best mean reward updated -13.360 -> -12.990, model saved\n",
            "159603: done 102 games, mean reward -12.650, eps 0.02, speed 57.30 f/s\n",
            "Best mean reward updated -12.990 -> -12.650, model saved\n",
            "162183: done 103 games, mean reward -12.380, eps 0.02, speed 57.31 f/s\n",
            "Best mean reward updated -12.650 -> -12.380, model saved\n",
            "164435: done 104 games, mean reward -12.050, eps 0.02, speed 57.63 f/s\n",
            "Best mean reward updated -12.380 -> -12.050, model saved\n",
            "166134: done 105 games, mean reward -11.640, eps 0.02, speed 57.49 f/s\n",
            "Best mean reward updated -12.050 -> -11.640, model saved\n",
            "167956: done 106 games, mean reward -11.240, eps 0.02, speed 57.91 f/s\n",
            "Best mean reward updated -11.640 -> -11.240, model saved\n",
            "169922: done 107 games, mean reward -10.860, eps 0.02, speed 57.73 f/s\n",
            "Best mean reward updated -11.240 -> -10.860, model saved\n",
            "172303: done 108 games, mean reward -10.530, eps 0.02, speed 57.67 f/s\n",
            "Best mean reward updated -10.860 -> -10.530, model saved\n",
            "174259: done 109 games, mean reward -10.160, eps 0.02, speed 57.49 f/s\n",
            "Best mean reward updated -10.530 -> -10.160, model saved\n",
            "175894: done 110 games, mean reward -9.740, eps 0.02, speed 57.26 f/s\n",
            "Best mean reward updated -10.160 -> -9.740, model saved\n",
            "177979: done 111 games, mean reward -9.370, eps 0.02, speed 57.01 f/s\n",
            "Best mean reward updated -9.740 -> -9.370, model saved\n",
            "179918: done 112 games, mean reward -8.990, eps 0.02, speed 57.16 f/s\n",
            "Best mean reward updated -9.370 -> -8.990, model saved\n",
            "181823: done 113 games, mean reward -8.600, eps 0.02, speed 57.12 f/s\n",
            "Best mean reward updated -8.990 -> -8.600, model saved\n",
            "183622: done 114 games, mean reward -8.210, eps 0.02, speed 57.07 f/s\n",
            "Best mean reward updated -8.600 -> -8.210, model saved\n",
            "185387: done 115 games, mean reward -7.810, eps 0.02, speed 57.03 f/s\n",
            "Best mean reward updated -8.210 -> -7.810, model saved\n",
            "187543: done 116 games, mean reward -7.440, eps 0.02, speed 57.83 f/s\n",
            "Best mean reward updated -7.810 -> -7.440, model saved\n",
            "189438: done 117 games, mean reward -7.060, eps 0.02, speed 57.66 f/s\n",
            "Best mean reward updated -7.440 -> -7.060, model saved\n",
            "191246: done 118 games, mean reward -6.660, eps 0.02, speed 58.10 f/s\n",
            "Best mean reward updated -7.060 -> -6.660, model saved\n",
            "193281: done 119 games, mean reward -6.310, eps 0.02, speed 57.78 f/s\n",
            "Best mean reward updated -6.660 -> -6.310, model saved\n",
            "195237: done 120 games, mean reward -5.920, eps 0.02, speed 57.71 f/s\n",
            "Best mean reward updated -6.310 -> -5.920, model saved\n",
            "197334: done 121 games, mean reward -5.560, eps 0.02, speed 57.26 f/s\n",
            "Best mean reward updated -5.920 -> -5.560, model saved\n",
            "199031: done 122 games, mean reward -5.150, eps 0.02, speed 57.19 f/s\n",
            "Best mean reward updated -5.560 -> -5.150, model saved\n",
            "200726: done 123 games, mean reward -4.740, eps 0.02, speed 57.17 f/s\n",
            "Best mean reward updated -5.150 -> -4.740, model saved\n",
            "202644: done 124 games, mean reward -4.350, eps 0.02, speed 57.14 f/s\n",
            "Best mean reward updated -4.740 -> -4.350, model saved\n",
            "204491: done 125 games, mean reward -3.970, eps 0.02, speed 57.14 f/s\n",
            "Best mean reward updated -4.350 -> -3.970, model saved\n",
            "206127: done 126 games, mean reward -3.550, eps 0.02, speed 56.80 f/s\n",
            "Best mean reward updated -3.970 -> -3.550, model saved\n",
            "208361: done 127 games, mean reward -3.170, eps 0.02, speed 57.32 f/s\n",
            "Best mean reward updated -3.550 -> -3.170, model saved\n",
            "209995: done 128 games, mean reward -2.750, eps 0.02, speed 57.88 f/s\n",
            "Best mean reward updated -3.170 -> -2.750, model saved\n",
            "211629: done 129 games, mean reward -2.350, eps 0.02, speed 57.46 f/s\n",
            "Best mean reward updated -2.750 -> -2.350, model saved\n",
            "213723: done 130 games, mean reward -2.000, eps 0.02, speed 57.92 f/s\n",
            "Best mean reward updated -2.350 -> -2.000, model saved\n",
            "215411: done 131 games, mean reward -1.610, eps 0.02, speed 57.67 f/s\n",
            "Best mean reward updated -2.000 -> -1.610, model saved\n",
            "217111: done 132 games, mean reward -1.230, eps 0.02, speed 57.77 f/s\n",
            "Best mean reward updated -1.610 -> -1.230, model saved\n",
            "219019: done 133 games, mean reward -0.840, eps 0.02, speed 57.47 f/s\n",
            "Best mean reward updated -1.230 -> -0.840, model saved\n",
            "221428: done 134 games, mean reward -0.510, eps 0.02, speed 57.12 f/s\n",
            "Best mean reward updated -0.840 -> -0.510, model saved\n",
            "223494: done 135 games, mean reward -0.110, eps 0.02, speed 57.15 f/s\n",
            "Best mean reward updated -0.510 -> -0.110, model saved\n",
            "225291: done 136 games, mean reward 0.300, eps 0.02, speed 57.48 f/s\n",
            "Best mean reward updated -0.110 -> 0.300, model saved\n",
            "227392: done 137 games, mean reward 0.680, eps 0.02, speed 57.25 f/s\n",
            "Best mean reward updated 0.300 -> 0.680, model saved\n",
            "229098: done 138 games, mean reward 1.070, eps 0.02, speed 57.31 f/s\n",
            "Best mean reward updated 0.680 -> 1.070, model saved\n",
            "230737: done 139 games, mean reward 1.460, eps 0.02, speed 57.79 f/s\n",
            "Best mean reward updated 1.070 -> 1.460, model saved\n",
            "232450: done 140 games, mean reward 1.850, eps 0.02, speed 57.66 f/s\n",
            "Best mean reward updated 1.460 -> 1.850, model saved\n",
            "234121: done 141 games, mean reward 2.250, eps 0.02, speed 57.89 f/s\n",
            "Best mean reward updated 1.850 -> 2.250, model saved\n",
            "235945: done 142 games, mean reward 2.650, eps 0.02, speed 57.85 f/s\n",
            "Best mean reward updated 2.250 -> 2.650, model saved\n",
            "237850: done 143 games, mean reward 3.050, eps 0.02, speed 57.34 f/s\n",
            "Best mean reward updated 2.650 -> 3.050, model saved\n",
            "239715: done 144 games, mean reward 3.390, eps 0.02, speed 58.09 f/s\n",
            "Best mean reward updated 3.050 -> 3.390, model saved\n",
            "241684: done 145 games, mean reward 3.780, eps 0.02, speed 58.40 f/s\n",
            "Best mean reward updated 3.390 -> 3.780, model saved\n",
            "243764: done 146 games, mean reward 4.120, eps 0.02, speed 58.39 f/s\n",
            "Best mean reward updated 3.780 -> 4.120, model saved\n",
            "246428: done 147 games, mean reward 4.400, eps 0.02, speed 58.31 f/s\n",
            "Best mean reward updated 4.120 -> 4.400, model saved\n",
            "248283: done 148 games, mean reward 4.790, eps 0.02, speed 57.97 f/s\n",
            "Best mean reward updated 4.400 -> 4.790, model saved\n",
            "250383: done 149 games, mean reward 5.120, eps 0.02, speed 58.27 f/s\n",
            "Best mean reward updated 4.790 -> 5.120, model saved\n",
            "252158: done 150 games, mean reward 5.480, eps 0.02, speed 58.24 f/s\n",
            "Best mean reward updated 5.120 -> 5.480, model saved\n",
            "253796: done 151 games, mean reward 5.890, eps 0.02, speed 58.33 f/s\n",
            "Best mean reward updated 5.480 -> 5.890, model saved\n",
            "255694: done 152 games, mean reward 6.250, eps 0.02, speed 57.85 f/s\n",
            "Best mean reward updated 5.890 -> 6.250, model saved\n",
            "257515: done 153 games, mean reward 6.650, eps 0.02, speed 58.15 f/s\n",
            "Best mean reward updated 6.250 -> 6.650, model saved\n",
            "259378: done 154 games, mean reward 7.030, eps 0.02, speed 58.17 f/s\n",
            "Best mean reward updated 6.650 -> 7.030, model saved\n",
            "261837: done 155 games, mean reward 7.360, eps 0.02, speed 58.00 f/s\n",
            "Best mean reward updated 7.030 -> 7.360, model saved\n",
            "263585: done 156 games, mean reward 7.730, eps 0.02, speed 57.96 f/s\n",
            "Best mean reward updated 7.360 -> 7.730, model saved\n",
            "265317: done 157 games, mean reward 8.110, eps 0.02, speed 57.75 f/s\n",
            "Best mean reward updated 7.730 -> 8.110, model saved\n",
            "267088: done 158 games, mean reward 8.480, eps 0.02, speed 58.23 f/s\n",
            "Best mean reward updated 8.110 -> 8.480, model saved\n",
            "269017: done 159 games, mean reward 8.860, eps 0.02, speed 58.09 f/s\n",
            "Best mean reward updated 8.480 -> 8.860, model saved\n",
            "270753: done 160 games, mean reward 9.220, eps 0.02, speed 57.99 f/s\n",
            "Best mean reward updated 8.860 -> 9.220, model saved\n",
            "272513: done 161 games, mean reward 9.560, eps 0.02, speed 58.18 f/s\n",
            "Best mean reward updated 9.220 -> 9.560, model saved\n",
            "274231: done 162 games, mean reward 9.930, eps 0.02, speed 57.69 f/s\n",
            "Best mean reward updated 9.560 -> 9.930, model saved\n",
            "276171: done 163 games, mean reward 10.240, eps 0.02, speed 57.78 f/s\n",
            "Best mean reward updated 9.930 -> 10.240, model saved\n",
            "278443: done 164 games, mean reward 10.570, eps 0.02, speed 57.83 f/s\n",
            "Best mean reward updated 10.240 -> 10.570, model saved\n",
            "280365: done 165 games, mean reward 10.930, eps 0.02, speed 58.18 f/s\n",
            "Best mean reward updated 10.570 -> 10.930, model saved\n",
            "283099: done 166 games, mean reward 11.210, eps 0.02, speed 57.89 f/s\n",
            "Best mean reward updated 10.930 -> 11.210, model saved\n",
            "284738: done 167 games, mean reward 11.600, eps 0.02, speed 57.95 f/s\n",
            "Best mean reward updated 11.210 -> 11.600, model saved\n",
            "286545: done 168 games, mean reward 11.980, eps 0.02, speed 58.18 f/s\n",
            "Best mean reward updated 11.600 -> 11.980, model saved\n",
            "288531: done 169 games, mean reward 12.320, eps 0.02, speed 58.64 f/s\n",
            "Best mean reward updated 11.980 -> 12.320, model saved\n",
            "290171: done 170 games, mean reward 12.700, eps 0.02, speed 58.58 f/s\n",
            "Best mean reward updated 12.320 -> 12.700, model saved\n",
            "291900: done 171 games, mean reward 13.070, eps 0.02, speed 57.99 f/s\n",
            "Best mean reward updated 12.700 -> 13.070, model saved\n",
            "294204: done 172 games, mean reward 13.300, eps 0.02, speed 58.12 f/s\n",
            "Best mean reward updated 13.070 -> 13.300, model saved\n",
            "297047: done 173 games, mean reward 13.470, eps 0.02, speed 58.21 f/s\n",
            "Best mean reward updated 13.300 -> 13.470, model saved\n",
            "299262: done 174 games, mean reward 13.750, eps 0.02, speed 58.47 f/s\n",
            "Best mean reward updated 13.470 -> 13.750, model saved\n",
            "301515: done 175 games, mean reward 13.990, eps 0.02, speed 58.55 f/s\n",
            "Best mean reward updated 13.750 -> 13.990, model saved\n",
            "303467: done 176 games, mean reward 14.290, eps 0.02, speed 58.67 f/s\n",
            "Best mean reward updated 13.990 -> 14.290, model saved\n",
            "305198: done 177 games, mean reward 14.640, eps 0.02, speed 58.57 f/s\n",
            "Best mean reward updated 14.290 -> 14.640, model saved\n",
            "306913: done 178 games, mean reward 14.980, eps 0.02, speed 58.73 f/s\n",
            "Best mean reward updated 14.640 -> 14.980, model saved\n",
            "308552: done 179 games, mean reward 15.320, eps 0.02, speed 58.79 f/s\n",
            "Best mean reward updated 14.980 -> 15.320, model saved\n",
            "310334: done 180 games, mean reward 15.640, eps 0.02, speed 58.25 f/s\n",
            "Best mean reward updated 15.320 -> 15.640, model saved\n",
            "312129: done 181 games, mean reward 15.860, eps 0.02, speed 58.61 f/s\n",
            "Best mean reward updated 15.640 -> 15.860, model saved\n",
            "313864: done 182 games, mean reward 16.070, eps 0.02, speed 58.55 f/s\n",
            "Best mean reward updated 15.860 -> 16.070, model saved\n",
            "315635: done 183 games, mean reward 16.200, eps 0.02, speed 58.36 f/s\n",
            "Best mean reward updated 16.070 -> 16.200, model saved\n",
            "317370: done 184 games, mean reward 16.480, eps 0.02, speed 58.44 f/s\n",
            "Best mean reward updated 16.200 -> 16.480, model saved\n",
            "319195: done 185 games, mean reward 16.790, eps 0.02, speed 58.65 f/s\n",
            "Best mean reward updated 16.480 -> 16.790, model saved\n",
            "320945: done 186 games, mean reward 17.130, eps 0.02, speed 58.43 f/s\n",
            "Best mean reward updated 16.790 -> 17.130, model saved\n",
            "322670: done 187 games, mean reward 17.380, eps 0.02, speed 58.57 f/s\n",
            "Best mean reward updated 17.130 -> 17.380, model saved\n",
            "324701: done 188 games, mean reward 17.460, eps 0.02, speed 58.59 f/s\n",
            "Best mean reward updated 17.380 -> 17.460, model saved\n",
            "326460: done 189 games, mean reward 17.540, eps 0.02, speed 58.52 f/s\n",
            "Best mean reward updated 17.460 -> 17.540, model saved\n",
            "328275: done 190 games, mean reward 17.550, eps 0.02, speed 58.19 f/s\n",
            "Best mean reward updated 17.540 -> 17.550, model saved\n",
            "330081: done 191 games, mean reward 17.570, eps 0.02, speed 58.52 f/s\n",
            "Best mean reward updated 17.550 -> 17.570, model saved\n",
            "331909: done 192 games, mean reward 17.590, eps 0.02, speed 58.72 f/s\n",
            "Best mean reward updated 17.570 -> 17.590, model saved\n",
            "333643: done 193 games, mean reward 17.620, eps 0.02, speed 58.72 f/s\n",
            "Best mean reward updated 17.590 -> 17.620, model saved\n",
            "335410: done 194 games, mean reward 17.710, eps 0.02, speed 58.56 f/s\n",
            "Best mean reward updated 17.620 -> 17.710, model saved\n",
            "337693: done 195 games, mean reward 17.670, eps 0.02, speed 58.68 f/s\n",
            "339660: done 196 games, mean reward 17.780, eps 0.02, speed 58.60 f/s\n",
            "Best mean reward updated 17.710 -> 17.780, model saved\n",
            "341563: done 197 games, mean reward 17.790, eps 0.02, speed 58.74 f/s\n",
            "Best mean reward updated 17.780 -> 17.790, model saved\n",
            "343331: done 198 games, mean reward 17.810, eps 0.02, speed 58.80 f/s\n",
            "Best mean reward updated 17.790 -> 17.810, model saved\n",
            "344969: done 199 games, mean reward 17.910, eps 0.02, speed 56.97 f/s\n",
            "Best mean reward updated 17.810 -> 17.910, model saved\n",
            "346703: done 200 games, mean reward 17.920, eps 0.02, speed 56.40 f/s\n",
            "Best mean reward updated 17.910 -> 17.920, model saved\n",
            "348401: done 201 games, mean reward 17.970, eps 0.02, speed 56.67 f/s\n",
            "Best mean reward updated 17.920 -> 17.970, model saved\n",
            "350203: done 202 games, mean reward 18.040, eps 0.02, speed 57.14 f/s\n",
            "Best mean reward updated 17.970 -> 18.040, model saved\n",
            "351842: done 203 games, mean reward 18.160, eps 0.02, speed 56.96 f/s\n",
            "Best mean reward updated 18.040 -> 18.160, model saved\n",
            "353706: done 204 games, mean reward 18.220, eps 0.02, speed 56.73 f/s\n",
            "Best mean reward updated 18.160 -> 18.220, model saved\n",
            "355557: done 205 games, mean reward 18.200, eps 0.02, speed 56.49 f/s\n",
            "357400: done 206 games, mean reward 18.190, eps 0.02, speed 56.47 f/s\n",
            "359407: done 207 games, mean reward 18.170, eps 0.02, speed 56.50 f/s\n",
            "361139: done 208 games, mean reward 18.250, eps 0.02, speed 56.44 f/s\n",
            "Best mean reward updated 18.220 -> 18.250, model saved\n",
            "362775: done 209 games, mean reward 18.300, eps 0.02, speed 56.87 f/s\n",
            "Best mean reward updated 18.250 -> 18.300, model saved\n",
            "364769: done 210 games, mean reward 18.260, eps 0.02, speed 56.49 f/s\n",
            "366475: done 211 games, mean reward 18.280, eps 0.02, speed 56.39 f/s\n",
            "368167: done 212 games, mean reward 18.300, eps 0.02, speed 56.44 f/s\n",
            "370046: done 213 games, mean reward 18.290, eps 0.02, speed 56.65 f/s\n",
            "371718: done 214 games, mean reward 18.300, eps 0.02, speed 56.70 f/s\n",
            "374076: done 215 games, mean reward 18.260, eps 0.02, speed 56.53 f/s\n",
            "376095: done 216 games, mean reward 18.230, eps 0.02, speed 56.48 f/s\n",
            "377852: done 217 games, mean reward 18.250, eps 0.02, speed 56.52 f/s\n",
            "379488: done 218 games, mean reward 18.270, eps 0.02, speed 56.65 f/s\n",
            "381124: done 219 games, mean reward 18.320, eps 0.02, speed 56.25 f/s\n",
            "Best mean reward updated 18.300 -> 18.320, model saved\n",
            "382813: done 220 games, mean reward 18.340, eps 0.02, speed 56.43 f/s\n",
            "Best mean reward updated 18.320 -> 18.340, model saved\n",
            "384544: done 221 games, mean reward 18.370, eps 0.02, speed 56.12 f/s\n",
            "Best mean reward updated 18.340 -> 18.370, model saved\n",
            "386277: done 222 games, mean reward 18.370, eps 0.02, speed 56.22 f/s\n",
            "387914: done 223 games, mean reward 18.370, eps 0.02, speed 56.06 f/s\n",
            "389815: done 224 games, mean reward 18.380, eps 0.02, speed 56.32 f/s\n",
            "Best mean reward updated 18.370 -> 18.380, model saved\n",
            "391724: done 225 games, mean reward 18.380, eps 0.02, speed 56.57 f/s\n",
            "393415: done 226 games, mean reward 18.370, eps 0.02, speed 57.53 f/s\n",
            "395340: done 227 games, mean reward 18.390, eps 0.02, speed 58.03 f/s\n",
            "Best mean reward updated 18.380 -> 18.390, model saved\n",
            "397263: done 228 games, mean reward 18.350, eps 0.02, speed 57.47 f/s\n",
            "399094: done 229 games, mean reward 18.330, eps 0.02, speed 56.63 f/s\n",
            "400800: done 230 games, mean reward 18.390, eps 0.02, speed 56.95 f/s\n",
            "402910: done 231 games, mean reward 18.330, eps 0.02, speed 56.91 f/s\n",
            "404907: done 232 games, mean reward 18.320, eps 0.02, speed 56.55 f/s\n",
            "406784: done 233 games, mean reward 18.320, eps 0.02, speed 56.43 f/s\n",
            "408535: done 234 games, mean reward 18.400, eps 0.02, speed 56.29 f/s\n",
            "Best mean reward updated 18.390 -> 18.400, model saved\n",
            "410556: done 235 games, mean reward 18.360, eps 0.02, speed 56.37 f/s\n",
            "412473: done 236 games, mean reward 18.330, eps 0.02, speed 56.43 f/s\n",
            "414555: done 237 games, mean reward 18.300, eps 0.02, speed 56.59 f/s\n",
            "416290: done 238 games, mean reward 18.300, eps 0.02, speed 56.49 f/s\n",
            "418060: done 239 games, mean reward 18.280, eps 0.02, speed 56.43 f/s\n",
            "419767: done 240 games, mean reward 18.270, eps 0.02, speed 56.60 f/s\n",
            "421580: done 241 games, mean reward 18.260, eps 0.02, speed 56.87 f/s\n",
            "423346: done 242 games, mean reward 18.260, eps 0.02, speed 56.78 f/s\n",
            "425400: done 243 games, mean reward 18.240, eps 0.02, speed 56.79 f/s\n",
            "427038: done 244 games, mean reward 18.290, eps 0.02, speed 56.63 f/s\n",
            "428990: done 245 games, mean reward 18.290, eps 0.02, speed 56.81 f/s\n",
            "430630: done 246 games, mean reward 18.340, eps 0.02, speed 56.86 f/s\n",
            "432320: done 247 games, mean reward 18.460, eps 0.02, speed 56.71 f/s\n",
            "Best mean reward updated 18.400 -> 18.460, model saved\n",
            "434173: done 248 games, mean reward 18.450, eps 0.02, speed 57.01 f/s\n",
            "435964: done 249 games, mean reward 18.480, eps 0.02, speed 56.76 f/s\n",
            "Best mean reward updated 18.460 -> 18.480, model saved\n",
            "437837: done 250 games, mean reward 18.460, eps 0.02, speed 56.94 f/s\n",
            "439535: done 251 games, mean reward 18.450, eps 0.02, speed 56.93 f/s\n",
            "441539: done 252 games, mean reward 18.400, eps 0.02, speed 56.97 f/s\n",
            "443535: done 253 games, mean reward 18.340, eps 0.02, speed 57.06 f/s\n",
            "445603: done 254 games, mean reward 18.290, eps 0.02, speed 57.93 f/s\n",
            "447334: done 255 games, mean reward 18.340, eps 0.02, speed 58.31 f/s\n",
            "449834: done 256 games, mean reward 18.260, eps 0.02, speed 58.18 f/s\n",
            "451683: done 257 games, mean reward 18.240, eps 0.02, speed 57.22 f/s\n",
            "453566: done 258 games, mean reward 18.250, eps 0.02, speed 56.87 f/s\n",
            "455204: done 259 games, mean reward 18.280, eps 0.02, speed 57.23 f/s\n",
            "457068: done 260 games, mean reward 18.280, eps 0.02, speed 57.42 f/s\n",
            "459012: done 261 games, mean reward 18.270, eps 0.02, speed 57.18 f/s\n",
            "461098: done 262 games, mean reward 18.260, eps 0.02, speed 57.21 f/s\n",
            "462735: done 263 games, mean reward 18.320, eps 0.02, speed 56.80 f/s\n",
            "464469: done 264 games, mean reward 18.370, eps 0.02, speed 56.63 f/s\n",
            "466109: done 265 games, mean reward 18.400, eps 0.02, speed 56.59 f/s\n",
            "467749: done 266 games, mean reward 18.500, eps 0.02, speed 56.75 f/s\n",
            "Best mean reward updated 18.480 -> 18.500, model saved\n",
            "469387: done 267 games, mean reward 18.500, eps 0.02, speed 56.54 f/s\n",
            "471165: done 268 games, mean reward 18.530, eps 0.02, speed 56.53 f/s\n",
            "Best mean reward updated 18.500 -> 18.530, model saved\n",
            "473041: done 269 games, mean reward 18.530, eps 0.02, speed 56.60 f/s\n",
            "474717: done 270 games, mean reward 18.520, eps 0.02, speed 56.61 f/s\n",
            "476661: done 271 games, mean reward 18.500, eps 0.02, speed 56.52 f/s\n",
            "478407: done 272 games, mean reward 18.560, eps 0.02, speed 56.58 f/s\n",
            "Best mean reward updated 18.530 -> 18.560, model saved\n",
            "480391: done 273 games, mean reward 18.680, eps 0.02, speed 56.42 f/s\n",
            "Best mean reward updated 18.560 -> 18.680, model saved\n",
            "482389: done 274 games, mean reward 18.680, eps 0.02, speed 56.82 f/s\n",
            "484152: done 275 games, mean reward 18.720, eps 0.02, speed 56.53 f/s\n",
            "Best mean reward updated 18.680 -> 18.720, model saved\n",
            "485920: done 276 games, mean reward 18.730, eps 0.02, speed 56.64 f/s\n",
            "Best mean reward updated 18.720 -> 18.730, model saved\n",
            "489094: done 277 games, mean reward 18.620, eps 0.02, speed 56.50 f/s\n",
            "490731: done 278 games, mean reward 18.620, eps 0.02, speed 56.53 f/s\n",
            "492958: done 279 games, mean reward 18.550, eps 0.02, speed 56.43 f/s\n",
            "495121: done 280 games, mean reward 18.510, eps 0.02, speed 56.41 f/s\n",
            "496901: done 281 games, mean reward 18.500, eps 0.02, speed 56.70 f/s\n",
            "498539: done 282 games, mean reward 18.510, eps 0.02, speed 57.52 f/s\n",
            "500175: done 283 games, mean reward 18.530, eps 0.02, speed 57.87 f/s\n",
            "502090: done 284 games, mean reward 18.520, eps 0.02, speed 57.91 f/s\n",
            "504423: done 285 games, mean reward 18.370, eps 0.02, speed 56.95 f/s\n",
            "506506: done 286 games, mean reward 18.340, eps 0.02, speed 56.64 f/s\n",
            "508361: done 287 games, mean reward 18.350, eps 0.02, speed 56.76 f/s\n",
            "510243: done 288 games, mean reward 18.330, eps 0.02, speed 56.76 f/s\n",
            "511944: done 289 games, mean reward 18.320, eps 0.02, speed 56.79 f/s\n",
            "513583: done 290 games, mean reward 18.350, eps 0.02, speed 56.79 f/s\n",
            "515301: done 291 games, mean reward 18.370, eps 0.02, speed 56.41 f/s\n",
            "517252: done 292 games, mean reward 18.350, eps 0.02, speed 56.55 f/s\n",
            "519567: done 293 games, mean reward 18.280, eps 0.02, speed 56.46 f/s\n",
            "521924: done 294 games, mean reward 18.240, eps 0.02, speed 56.51 f/s\n",
            "523563: done 295 games, mean reward 18.330, eps 0.02, speed 56.21 f/s\n",
            "525324: done 296 games, mean reward 18.350, eps 0.02, speed 56.57 f/s\n",
            "527136: done 297 games, mean reward 18.380, eps 0.02, speed 56.60 f/s\n",
            "528935: done 298 games, mean reward 18.380, eps 0.02, speed 56.41 f/s\n",
            "530609: done 299 games, mean reward 18.370, eps 0.02, speed 56.89 f/s\n",
            "532300: done 300 games, mean reward 18.370, eps 0.02, speed 57.13 f/s\n",
            "533940: done 301 games, mean reward 18.370, eps 0.02, speed 56.87 f/s\n",
            "535629: done 302 games, mean reward 18.370, eps 0.02, speed 57.08 f/s\n",
            "537381: done 303 games, mean reward 18.360, eps 0.02, speed 57.13 f/s\n",
            "539343: done 304 games, mean reward 18.340, eps 0.02, speed 56.96 f/s\n",
            "541224: done 305 games, mean reward 18.340, eps 0.02, speed 56.67 f/s\n",
            "543628: done 306 games, mean reward 18.310, eps 0.02, speed 56.73 f/s\n",
            "545939: done 307 games, mean reward 18.310, eps 0.02, speed 56.62 f/s\n",
            "548000: done 308 games, mean reward 18.290, eps 0.02, speed 56.80 f/s\n",
            "550039: done 309 games, mean reward 18.260, eps 0.02, speed 57.12 f/s\n",
            "551763: done 310 games, mean reward 18.280, eps 0.02, speed 58.27 f/s\n",
            "553400: done 311 games, mean reward 18.300, eps 0.02, speed 58.05 f/s\n",
            "555122: done 312 games, mean reward 18.300, eps 0.02, speed 57.81 f/s\n",
            "556793: done 313 games, mean reward 18.330, eps 0.02, speed 56.50 f/s\n",
            "558523: done 314 games, mean reward 18.330, eps 0.02, speed 56.06 f/s\n",
            "560264: done 315 games, mean reward 18.370, eps 0.02, speed 56.27 f/s\n",
            "562528: done 316 games, mean reward 18.390, eps 0.02, speed 56.37 f/s\n",
            "564377: done 317 games, mean reward 18.380, eps 0.02, speed 56.48 f/s\n",
            "566312: done 318 games, mean reward 18.350, eps 0.02, speed 56.56 f/s\n",
            "567949: done 319 games, mean reward 18.350, eps 0.02, speed 56.33 f/s\n",
            "569707: done 320 games, mean reward 18.360, eps 0.02, speed 56.23 f/s\n",
            "571461: done 321 games, mean reward 18.350, eps 0.02, speed 56.34 f/s\n",
            "573416: done 322 games, mean reward 18.310, eps 0.02, speed 56.26 f/s\n",
            "575374: done 323 games, mean reward 18.270, eps 0.02, speed 56.40 f/s\n",
            "577013: done 324 games, mean reward 18.290, eps 0.02, speed 56.50 f/s\n",
            "578650: done 325 games, mean reward 18.320, eps 0.02, speed 56.38 f/s\n",
            "580287: done 326 games, mean reward 18.330, eps 0.02, speed 56.62 f/s\n",
            "582020: done 327 games, mean reward 18.340, eps 0.02, speed 56.35 f/s\n",
            "583984: done 328 games, mean reward 18.350, eps 0.02, speed 56.49 f/s\n",
            "585870: done 329 games, mean reward 18.340, eps 0.02, speed 56.34 f/s\n",
            "587524: done 330 games, mean reward 18.350, eps 0.02, speed 56.39 f/s\n",
            "589161: done 331 games, mean reward 18.420, eps 0.02, speed 56.18 f/s\n",
            "590800: done 332 games, mean reward 18.440, eps 0.02, speed 56.41 f/s\n",
            "592473: done 333 games, mean reward 18.460, eps 0.02, speed 56.47 f/s\n",
            "594206: done 334 games, mean reward 18.460, eps 0.02, speed 56.29 f/s\n",
            "595953: done 335 games, mean reward 18.510, eps 0.02, speed 56.32 f/s\n",
            "597839: done 336 games, mean reward 18.510, eps 0.02, speed 56.38 f/s\n",
            "599513: done 337 games, mean reward 18.570, eps 0.02, speed 56.24 f/s\n",
            "601150: done 338 games, mean reward 18.580, eps 0.02, speed 56.63 f/s\n",
            "602787: done 339 games, mean reward 18.600, eps 0.02, speed 56.43 f/s\n",
            "604521: done 340 games, mean reward 18.600, eps 0.02, speed 57.87 f/s\n",
            "606618: done 341 games, mean reward 18.540, eps 0.02, speed 57.78 f/s\n",
            "608581: done 342 games, mean reward 18.520, eps 0.02, speed 57.61 f/s\n",
            "610440: done 343 games, mean reward 18.520, eps 0.02, speed 56.46 f/s\n",
            "612170: done 344 games, mean reward 18.510, eps 0.02, speed 55.94 f/s\n",
            "613805: done 345 games, mean reward 18.540, eps 0.02, speed 55.54 f/s\n",
            "615596: done 346 games, mean reward 18.530, eps 0.02, speed 56.28 f/s\n",
            "617231: done 347 games, mean reward 18.540, eps 0.02, speed 56.82 f/s\n",
            "618866: done 348 games, mean reward 18.570, eps 0.02, speed 56.97 f/s\n",
            "620678: done 349 games, mean reward 18.580, eps 0.02, speed 56.85 f/s\n",
            "622318: done 350 games, mean reward 18.610, eps 0.02, speed 56.59 f/s\n",
            "624167: done 351 games, mean reward 18.590, eps 0.02, speed 56.43 f/s\n",
            "625876: done 352 games, mean reward 18.660, eps 0.02, speed 56.19 f/s\n",
            "627732: done 353 games, mean reward 18.710, eps 0.02, speed 56.27 f/s\n",
            "629371: done 354 games, mean reward 18.790, eps 0.02, speed 56.09 f/s\n",
            "Best mean reward updated 18.730 -> 18.790, model saved\n",
            "631008: done 355 games, mean reward 18.800, eps 0.02, speed 56.34 f/s\n",
            "Best mean reward updated 18.790 -> 18.800, model saved\n",
            "632805: done 356 games, mean reward 18.860, eps 0.02, speed 55.99 f/s\n",
            "Best mean reward updated 18.800 -> 18.860, model saved\n",
            "634590: done 357 games, mean reward 18.870, eps 0.02, speed 56.22 f/s\n",
            "Best mean reward updated 18.860 -> 18.870, model saved\n",
            "636329: done 358 games, mean reward 18.870, eps 0.02, speed 56.13 f/s\n",
            "638048: done 359 games, mean reward 18.860, eps 0.02, speed 56.21 f/s\n",
            "639742: done 360 games, mean reward 18.870, eps 0.02, speed 56.20 f/s\n",
            "641433: done 361 games, mean reward 18.890, eps 0.02, speed 56.23 f/s\n",
            "Best mean reward updated 18.870 -> 18.890, model saved\n",
            "643071: done 362 games, mean reward 18.910, eps 0.02, speed 57.30 f/s\n",
            "Best mean reward updated 18.890 -> 18.910, model saved\n",
            "644706: done 363 games, mean reward 18.910, eps 0.02, speed 58.67 f/s\n",
            "646496: done 364 games, mean reward 18.900, eps 0.02, speed 58.57 f/s\n",
            "648137: done 365 games, mean reward 18.900, eps 0.02, speed 58.70 f/s\n",
            "649826: done 366 games, mean reward 18.890, eps 0.02, speed 58.65 f/s\n",
            "651591: done 367 games, mean reward 18.870, eps 0.02, speed 58.77 f/s\n",
            "653505: done 368 games, mean reward 18.850, eps 0.02, speed 58.73 f/s\n",
            "655144: done 369 games, mean reward 18.880, eps 0.02, speed 58.74 f/s\n",
            "656866: done 370 games, mean reward 18.880, eps 0.02, speed 58.81 f/s\n",
            "658505: done 371 games, mean reward 18.900, eps 0.02, speed 58.90 f/s\n",
            "660206: done 372 games, mean reward 18.910, eps 0.02, speed 58.83 f/s\n",
            "662180: done 373 games, mean reward 18.890, eps 0.02, speed 58.79 f/s\n",
            "663834: done 374 games, mean reward 18.940, eps 0.02, speed 58.93 f/s\n",
            "Best mean reward updated 18.910 -> 18.940, model saved\n",
            "665683: done 375 games, mean reward 18.930, eps 0.02, speed 58.73 f/s\n",
            "667394: done 376 games, mean reward 18.940, eps 0.02, speed 58.75 f/s\n",
            "669087: done 377 games, mean reward 19.050, eps 0.02, speed 58.70 f/s\n",
            "Best mean reward updated 18.940 -> 19.050, model saved\n",
            "670926: done 378 games, mean reward 19.040, eps 0.02, speed 57.85 f/s\n",
            "672565: done 379 games, mean reward 19.110, eps 0.02, speed 57.98 f/s\n",
            "Best mean reward updated 19.050 -> 19.110, model saved\n",
            "674644: done 380 games, mean reward 19.060, eps 0.02, speed 58.04 f/s\n",
            "676400: done 381 games, mean reward 19.060, eps 0.02, speed 58.18 f/s\n",
            "678039: done 382 games, mean reward 19.060, eps 0.02, speed 58.22 f/s\n",
            "679762: done 383 games, mean reward 19.050, eps 0.02, speed 58.17 f/s\n",
            "681483: done 384 games, mean reward 19.060, eps 0.02, speed 58.20 f/s\n",
            "683368: done 385 games, mean reward 19.190, eps 0.02, speed 58.03 f/s\n",
            "Best mean reward updated 19.110 -> 19.190, model saved\n",
            "685249: done 386 games, mean reward 19.170, eps 0.02, speed 58.21 f/s\n",
            "686940: done 387 games, mean reward 19.170, eps 0.02, speed 58.44 f/s\n",
            "688755: done 388 games, mean reward 19.200, eps 0.02, speed 58.51 f/s\n",
            "Best mean reward updated 19.190 -> 19.200, model saved\n",
            "690391: done 389 games, mean reward 19.210, eps 0.02, speed 58.18 f/s\n",
            "Best mean reward updated 19.200 -> 19.210, model saved\n",
            "692127: done 390 games, mean reward 19.200, eps 0.02, speed 58.24 f/s\n",
            "693966: done 391 games, mean reward 19.180, eps 0.02, speed 58.30 f/s\n",
            "695633: done 392 games, mean reward 19.210, eps 0.02, speed 58.62 f/s\n",
            "697352: done 393 games, mean reward 19.280, eps 0.02, speed 58.61 f/s\n",
            "Best mean reward updated 19.210 -> 19.280, model saved\n",
            "699027: done 394 games, mean reward 19.330, eps 0.02, speed 58.58 f/s\n",
            "Best mean reward updated 19.280 -> 19.330, model saved\n",
            "700878: done 395 games, mean reward 19.290, eps 0.02, speed 58.28 f/s\n",
            "702570: done 396 games, mean reward 19.280, eps 0.02, speed 58.57 f/s\n",
            "704211: done 397 games, mean reward 19.290, eps 0.02, speed 58.87 f/s\n",
            "706002: done 398 games, mean reward 19.280, eps 0.02, speed 58.71 f/s\n",
            "707764: done 399 games, mean reward 19.270, eps 0.02, speed 58.81 f/s\n",
            "709467: done 400 games, mean reward 19.270, eps 0.02, speed 58.75 f/s\n",
            "711106: done 401 games, mean reward 19.270, eps 0.02, speed 58.81 f/s\n",
            "712934: done 402 games, mean reward 19.270, eps 0.02, speed 58.70 f/s\n",
            "715113: done 403 games, mean reward 19.200, eps 0.02, speed 58.77 f/s\n",
            "717130: done 404 games, mean reward 19.190, eps 0.02, speed 58.82 f/s\n",
            "718871: done 405 games, mean reward 19.210, eps 0.02, speed 58.51 f/s\n",
            "720924: done 406 games, mean reward 19.180, eps 0.02, speed 58.72 f/s\n",
            "722579: done 407 games, mean reward 19.230, eps 0.02, speed 58.65 f/s\n",
            "724217: done 408 games, mean reward 19.260, eps 0.02, speed 58.67 f/s\n",
            "725856: done 409 games, mean reward 19.290, eps 0.02, speed 58.71 f/s\n",
            "727494: done 410 games, mean reward 19.310, eps 0.02, speed 58.77 f/s\n",
            "729129: done 411 games, mean reward 19.310, eps 0.02, speed 58.79 f/s\n",
            "730834: done 412 games, mean reward 19.310, eps 0.02, speed 58.68 f/s\n",
            "732557: done 413 games, mean reward 19.310, eps 0.02, speed 58.74 f/s\n",
            "734288: done 414 games, mean reward 19.310, eps 0.02, speed 59.59 f/s\n",
            "735925: done 415 games, mean reward 19.330, eps 0.02, speed 60.12 f/s\n",
            "737756: done 416 games, mean reward 19.360, eps 0.02, speed 59.90 f/s\n",
            "Best mean reward updated 19.330 -> 19.360, model saved\n",
            "739488: done 417 games, mean reward 19.380, eps 0.02, speed 60.03 f/s\n",
            "Best mean reward updated 19.360 -> 19.380, model saved\n",
            "741257: done 418 games, mean reward 19.390, eps 0.02, speed 60.02 f/s\n",
            "Best mean reward updated 19.380 -> 19.390, model saved\n",
            "743096: done 419 games, mean reward 19.360, eps 0.02, speed 60.10 f/s\n",
            "744833: done 420 games, mean reward 19.350, eps 0.02, speed 60.11 f/s\n",
            "746472: done 421 games, mean reward 19.370, eps 0.02, speed 60.20 f/s\n",
            "748371: done 422 games, mean reward 19.390, eps 0.02, speed 60.08 f/s\n",
            "750062: done 423 games, mean reward 19.420, eps 0.02, speed 59.97 f/s\n",
            "Best mean reward updated 19.390 -> 19.420, model saved\n",
            "751751: done 424 games, mean reward 19.410, eps 0.02, speed 60.08 f/s\n",
            "753483: done 425 games, mean reward 19.400, eps 0.02, speed 60.19 f/s\n",
            "755197: done 426 games, mean reward 19.400, eps 0.02, speed 60.18 f/s\n",
            "756916: done 427 games, mean reward 19.400, eps 0.02, speed 59.82 f/s\n",
            "758827: done 428 games, mean reward 19.400, eps 0.02, speed 60.03 f/s\n",
            "760518: done 429 games, mean reward 19.420, eps 0.02, speed 60.19 f/s\n",
            "762252: done 430 games, mean reward 19.410, eps 0.02, speed 60.20 f/s\n",
            "763888: done 431 games, mean reward 19.410, eps 0.02, speed 60.15 f/s\n",
            "765647: done 432 games, mean reward 19.390, eps 0.02, speed 60.08 f/s\n",
            "767449: done 433 games, mean reward 19.380, eps 0.02, speed 59.98 f/s\n",
            "769202: done 434 games, mean reward 19.370, eps 0.02, speed 60.06 f/s\n",
            "771075: done 435 games, mean reward 19.360, eps 0.02, speed 60.25 f/s\n",
            "773005: done 436 games, mean reward 19.360, eps 0.02, speed 60.18 f/s\n",
            "774646: done 437 games, mean reward 19.370, eps 0.02, speed 59.75 f/s\n",
            "776553: done 438 games, mean reward 19.350, eps 0.02, speed 60.06 f/s\n",
            "778307: done 439 games, mean reward 19.330, eps 0.02, speed 60.14 f/s\n",
            "779944: done 440 games, mean reward 19.340, eps 0.02, speed 60.02 f/s\n",
            "781679: done 441 games, mean reward 19.410, eps 0.02, speed 60.10 f/s\n",
            "783317: done 442 games, mean reward 19.450, eps 0.02, speed 60.06 f/s\n",
            "Best mean reward updated 19.420 -> 19.450, model saved\n",
            "785260: done 443 games, mean reward 19.460, eps 0.02, speed 60.22 f/s\n",
            "Best mean reward updated 19.450 -> 19.460, model saved\n",
            "787037: done 444 games, mean reward 19.450, eps 0.02, speed 60.05 f/s\n",
            "788678: done 445 games, mean reward 19.450, eps 0.02, speed 60.01 f/s\n",
            "790534: done 446 games, mean reward 19.450, eps 0.02, speed 60.02 f/s\n",
            "792331: done 447 games, mean reward 19.420, eps 0.02, speed 60.14 f/s\n",
            "794308: done 448 games, mean reward 19.410, eps 0.02, speed 59.75 f/s\n",
            "796223: done 449 games, mean reward 19.390, eps 0.02, speed 60.12 f/s\n",
            "797965: done 450 games, mean reward 19.370, eps 0.02, speed 60.00 f/s\n",
            "799877: done 451 games, mean reward 19.370, eps 0.02, speed 60.02 f/s\n",
            "801517: done 452 games, mean reward 19.380, eps 0.02, speed 59.99 f/s\n",
            "803304: done 453 games, mean reward 19.390, eps 0.02, speed 60.12 f/s\n",
            "804943: done 454 games, mean reward 19.390, eps 0.02, speed 60.09 f/s\n",
            "806800: done 455 games, mean reward 19.380, eps 0.02, speed 60.06 f/s\n",
            "808870: done 456 games, mean reward 19.360, eps 0.02, speed 60.23 f/s\n",
            "810744: done 457 games, mean reward 19.360, eps 0.02, speed 59.98 f/s\n",
            "812382: done 458 games, mean reward 19.370, eps 0.02, speed 59.66 f/s\n",
            "814146: done 459 games, mean reward 19.360, eps 0.02, speed 60.06 f/s\n",
            "815932: done 460 games, mean reward 19.360, eps 0.02, speed 60.23 f/s\n",
            "817606: done 461 games, mean reward 19.360, eps 0.02, speed 60.14 f/s\n",
            "819297: done 462 games, mean reward 19.350, eps 0.02, speed 60.13 f/s\n",
            "821238: done 463 games, mean reward 19.300, eps 0.02, speed 60.00 f/s\n",
            "822960: done 464 games, mean reward 19.310, eps 0.02, speed 59.95 f/s\n",
            "824694: done 465 games, mean reward 19.300, eps 0.02, speed 60.05 f/s\n",
            "826713: done 466 games, mean reward 19.260, eps 0.02, speed 59.98 f/s\n",
            "828447: done 467 games, mean reward 19.270, eps 0.02, speed 59.99 f/s\n",
            "830289: done 468 games, mean reward 19.260, eps 0.02, speed 59.79 f/s\n",
            "832106: done 469 games, mean reward 19.240, eps 0.02, speed 59.95 f/s\n",
            "833999: done 470 games, mean reward 19.230, eps 0.02, speed 60.15 f/s\n",
            "835758: done 471 games, mean reward 19.220, eps 0.02, speed 60.15 f/s\n",
            "837474: done 472 games, mean reward 19.230, eps 0.02, speed 60.05 f/s\n",
            "839290: done 473 games, mean reward 19.270, eps 0.02, speed 60.01 f/s\n",
            "840929: done 474 games, mean reward 19.270, eps 0.02, speed 60.15 f/s\n",
            "842566: done 475 games, mean reward 19.290, eps 0.02, speed 60.07 f/s\n",
            "844480: done 476 games, mean reward 19.270, eps 0.02, speed 60.05 f/s\n",
            "846275: done 477 games, mean reward 19.260, eps 0.02, speed 59.96 f/s\n",
            "848010: done 478 games, mean reward 19.260, eps 0.02, speed 59.83 f/s\n",
            "849866: done 479 games, mean reward 19.240, eps 0.02, speed 59.82 f/s\n",
            "851828: done 480 games, mean reward 19.320, eps 0.02, speed 60.05 f/s\n",
            "853620: done 481 games, mean reward 19.320, eps 0.02, speed 60.09 f/s\n",
            "855344: done 482 games, mean reward 19.310, eps 0.02, speed 59.67 f/s\n",
            "857078: done 483 games, mean reward 19.310, eps 0.02, speed 60.07 f/s\n",
            "858718: done 484 games, mean reward 19.320, eps 0.02, speed 59.68 f/s\n",
            "860566: done 485 games, mean reward 19.330, eps 0.02, speed 60.15 f/s\n",
            "862379: done 486 games, mean reward 19.380, eps 0.02, speed 60.02 f/s\n",
            "864205: done 487 games, mean reward 19.380, eps 0.02, speed 60.07 f/s\n",
            "865852: done 488 games, mean reward 19.400, eps 0.02, speed 59.91 f/s\n",
            "867739: done 489 games, mean reward 19.370, eps 0.02, speed 59.78 f/s\n",
            "869819: done 490 games, mean reward 19.310, eps 0.02, speed 59.99 f/s\n",
            "871553: done 491 games, mean reward 19.320, eps 0.02, speed 60.16 f/s\n",
            "873301: done 492 games, mean reward 19.320, eps 0.02, speed 60.01 f/s\n",
            "874973: done 493 games, mean reward 19.320, eps 0.02, speed 59.98 f/s\n",
            "876664: done 494 games, mean reward 19.320, eps 0.02, speed 60.00 f/s\n",
            "878400: done 495 games, mean reward 19.350, eps 0.02, speed 60.10 f/s\n",
            "880038: done 496 games, mean reward 19.360, eps 0.02, speed 60.07 f/s\n",
            "881758: done 497 games, mean reward 19.350, eps 0.02, speed 60.08 f/s\n",
            "883452: done 498 games, mean reward 19.370, eps 0.02, speed 60.02 f/s\n",
            "885204: done 499 games, mean reward 19.380, eps 0.02, speed 60.05 f/s\n",
            "886906: done 500 games, mean reward 19.380, eps 0.02, speed 59.82 f/s\n",
            "888607: done 501 games, mean reward 19.370, eps 0.02, speed 60.12 f/s\n",
            "890423: done 502 games, mean reward 19.360, eps 0.02, speed 60.02 f/s\n",
            "892155: done 503 games, mean reward 19.440, eps 0.02, speed 60.07 f/s\n",
            "893846: done 504 games, mean reward 19.480, eps 0.02, speed 60.12 f/s\n",
            "Best mean reward updated 19.460 -> 19.480, model saved\n",
            "895488: done 505 games, mean reward 19.490, eps 0.02, speed 60.03 f/s\n",
            "Best mean reward updated 19.480 -> 19.490, model saved\n",
            "897330: done 506 games, mean reward 19.550, eps 0.02, speed 60.02 f/s\n",
            "Best mean reward updated 19.490 -> 19.550, model saved\n",
            "898971: done 507 games, mean reward 19.550, eps 0.02, speed 60.02 f/s\n",
            "900732: done 508 games, mean reward 19.530, eps 0.02, speed 60.11 f/s\n",
            "902922: done 509 games, mean reward 19.400, eps 0.02, speed 60.06 f/s\n",
            "904559: done 510 games, mean reward 19.400, eps 0.02, speed 60.12 f/s\n",
            "906195: done 511 games, mean reward 19.400, eps 0.02, speed 59.94 f/s\n",
            "907918: done 512 games, mean reward 19.390, eps 0.02, speed 60.12 f/s\n",
            "909673: done 513 games, mean reward 19.390, eps 0.02, speed 60.04 f/s\n",
            "911452: done 514 games, mean reward 19.380, eps 0.02, speed 60.07 f/s\n",
            "913276: done 515 games, mean reward 19.350, eps 0.02, speed 60.09 f/s\n",
            "914916: done 516 games, mean reward 19.380, eps 0.02, speed 59.99 f/s\n",
            "916772: done 517 games, mean reward 19.360, eps 0.02, speed 60.19 f/s\n",
            "918488: done 518 games, mean reward 19.370, eps 0.02, speed 60.02 f/s\n",
            "920231: done 519 games, mean reward 19.380, eps 0.02, speed 60.05 f/s\n",
            "921963: done 520 games, mean reward 19.390, eps 0.02, speed 60.12 f/s\n",
            "923600: done 521 games, mean reward 19.390, eps 0.02, speed 59.98 f/s\n",
            "925359: done 522 games, mean reward 19.410, eps 0.02, speed 60.17 f/s\n",
            "927191: done 523 games, mean reward 19.410, eps 0.02, speed 60.04 f/s\n",
            "928830: done 524 games, mean reward 19.420, eps 0.02, speed 60.15 f/s\n",
            "930486: done 525 games, mean reward 19.430, eps 0.02, speed 60.22 f/s\n",
            "932300: done 526 games, mean reward 19.420, eps 0.02, speed 60.13 f/s\n",
            "934006: done 527 games, mean reward 19.420, eps 0.02, speed 60.00 f/s\n",
            "935698: done 528 games, mean reward 19.440, eps 0.02, speed 60.06 f/s\n",
            "937513: done 529 games, mean reward 19.430, eps 0.02, speed 60.19 f/s\n",
            "939237: done 530 games, mean reward 19.430, eps 0.02, speed 60.19 f/s\n",
            "940874: done 531 games, mean reward 19.430, eps 0.02, speed 59.95 f/s\n",
            "942512: done 532 games, mean reward 19.450, eps 0.02, speed 60.09 f/s\n",
            "944150: done 533 games, mean reward 19.470, eps 0.02, speed 60.04 f/s\n",
            "945956: done 534 games, mean reward 19.460, eps 0.02, speed 60.10 f/s\n",
            "947595: done 535 games, mean reward 19.480, eps 0.02, speed 60.14 f/s\n",
            "949512: done 536 games, mean reward 19.480, eps 0.02, speed 60.17 f/s\n",
            "951345: done 537 games, mean reward 19.460, eps 0.02, speed 60.08 f/s\n",
            "952991: done 538 games, mean reward 19.480, eps 0.02, speed 59.92 f/s\n",
            "955089: done 539 games, mean reward 19.460, eps 0.02, speed 60.00 f/s\n",
            "956876: done 540 games, mean reward 19.440, eps 0.02, speed 60.00 f/s\n",
            "958570: done 541 games, mean reward 19.440, eps 0.02, speed 59.85 f/s\n",
            "960276: done 542 games, mean reward 19.430, eps 0.02, speed 60.09 f/s\n",
            "961912: done 543 games, mean reward 19.460, eps 0.02, speed 59.82 f/s\n",
            "963603: done 544 games, mean reward 19.470, eps 0.02, speed 60.01 f/s\n",
            "965344: done 545 games, mean reward 19.460, eps 0.02, speed 59.99 f/s\n",
            "966979: done 546 games, mean reward 19.470, eps 0.02, speed 59.96 f/s\n",
            "968733: done 547 games, mean reward 19.490, eps 0.02, speed 59.89 f/s\n",
            "970425: done 548 games, mean reward 19.490, eps 0.02, speed 59.95 f/s\n",
            "972123: done 549 games, mean reward 19.510, eps 0.02, speed 59.82 f/s\n",
            "973899: done 550 games, mean reward 19.510, eps 0.02, speed 59.88 f/s\n",
            "975542: done 551 games, mean reward 19.540, eps 0.02, speed 59.84 f/s\n",
            "977179: done 552 games, mean reward 19.540, eps 0.02, speed 59.76 f/s\n",
            "979029: done 553 games, mean reward 19.530, eps 0.02, speed 60.01 f/s\n",
            "980756: done 554 games, mean reward 19.510, eps 0.02, speed 59.62 f/s\n",
            "982527: done 555 games, mean reward 19.500, eps 0.02, speed 59.74 f/s\n",
            "984170: done 556 games, mean reward 19.550, eps 0.02, speed 59.98 f/s\n",
            "985806: done 557 games, mean reward 19.570, eps 0.02, speed 59.95 f/s\n",
            "Best mean reward updated 19.550 -> 19.570, model saved\n",
            "987447: done 558 games, mean reward 19.570, eps 0.02, speed 59.84 f/s\n",
            "989213: done 559 games, mean reward 19.570, eps 0.02, speed 59.95 f/s\n",
            "991028: done 560 games, mean reward 19.540, eps 0.02, speed 59.93 f/s\n",
            "992679: done 561 games, mean reward 19.550, eps 0.02, speed 60.04 f/s\n",
            "994442: done 562 games, mean reward 19.550, eps 0.02, speed 60.02 f/s\n",
            "996082: done 563 games, mean reward 19.600, eps 0.02, speed 59.84 f/s\n",
            "Best mean reward updated 19.570 -> 19.600, model saved\n",
            "997721: done 564 games, mean reward 19.610, eps 0.02, speed 59.88 f/s\n",
            "Best mean reward updated 19.600 -> 19.610, model saved\n",
            "999661: done 565 games, mean reward 19.590, eps 0.02, speed 59.70 f/s\n",
            "1001352: done 566 games, mean reward 19.630, eps 0.02, speed 60.15 f/s\n",
            "Best mean reward updated 19.610 -> 19.630, model saved\n",
            "1003208: done 567 games, mean reward 19.600, eps 0.02, speed 59.97 f/s\n",
            "1004870: done 568 games, mean reward 19.630, eps 0.02, speed 60.24 f/s\n",
            "1006766: done 569 games, mean reward 19.620, eps 0.02, speed 60.07 f/s\n",
            "1008760: done 570 games, mean reward 19.580, eps 0.02, speed 60.13 f/s\n",
            "1010397: done 571 games, mean reward 19.590, eps 0.02, speed 60.16 f/s\n",
            "1012315: done 572 games, mean reward 19.560, eps 0.02, speed 60.02 f/s\n",
            "1014035: done 573 games, mean reward 19.580, eps 0.02, speed 59.99 f/s\n",
            "1015690: done 574 games, mean reward 19.580, eps 0.02, speed 60.11 f/s\n",
            "1017364: done 575 games, mean reward 19.570, eps 0.02, speed 59.78 f/s\n",
            "1019096: done 576 games, mean reward 19.580, eps 0.02, speed 58.81 f/s\n",
            "1021067: done 577 games, mean reward 19.550, eps 0.02, speed 59.65 f/s\n",
            "1022790: done 578 games, mean reward 19.550, eps 0.02, speed 59.55 f/s\n",
            "1024547: done 579 games, mean reward 19.560, eps 0.02, speed 59.61 f/s\n",
            "1026414: done 580 games, mean reward 19.570, eps 0.02, speed 59.55 f/s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-388d9c2f2457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4998114fbb8d>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjONOyoxstC"
      },
      "source": [
        "## 評価\n",
        "学習したネットワーク（エージェント）を確認してみます．\n",
        "\n",
        "ここでは，framesに描画したフレームを順次格納します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14MSs0oPeXl2"
      },
      "source": [
        "# 結果を描画するための設定\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display._obj._screen)\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state_a).to(device).float()\n",
        "    done = False\n",
        "    R = 0\n",
        "    t = 0\n",
        "    \n",
        "    while not done and t < 200:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        action = net(state)\n",
        "        state, r, done, _ = env.step(action.max(1)[1].view(1, 1).item())\n",
        "        state = torch.from_numpy(state).to(device).float()\n",
        "        R += r\n",
        "        t += 1\n",
        "    print(\"test episode:\", i, \"R:\", R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5tJoQekxstF"
      },
      "source": [
        "## 描画\n",
        "\n",
        "maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFDCxq1GeXl4"
      },
      "source": [
        "# 実行結果の表示\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZDh4S6m2hK0"
      },
      "source": [
        "## 課題"
      ]
    }
  ]
}